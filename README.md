# MSCS_634_Lab_2

# Lab Assignment 2: Classification Using KNN and RNN Algorithms

*Author*: Murali Krishna Chintha  
*Course*: MSCS-634-M40 - Advanced Big Data and Data Mining  

## ğŸ“Œ Purpose
The goal of this lab is to apply two supervised machine learning algorithmsâ€”K-Nearest Neighbors (KNN) and Radius Neighbors (RNN)â€”on the Wine dataset from â â€¯sklearnâ€¯â . This assignment demonstrates how different values of â â€¯kâ€¯â  and â â€¯radiusâ€¯â  affect classification accuracy, and helps understand the practical differences between these two neighbor-based methods.

## ğŸ“Š Key Insights
â€¢â   â *KNN*:
  - Accuracy improved from â â€¯k=1â€¯â  to â â€¯k=11â€¯â  but started to drop slightly beyond that.
  - A moderate value of â â€¯k=5â€¯â  or â â€¯k=11â€¯â  yielded the best balance between bias and variance.

â€¢â   â *RNN*:
  - Performance was highly sensitive to the radius value.
  - Too small a radius led to poor coverage (some test samples had no neighbors and were not classified).
  - Mid-range values like â â€¯radius=450â€¯â  gave better and more stable results.

â€¢â   â *Comparison*:
  - KNN consistently classified all test samples, making it more reliable for smaller datasets.
  - RNN showed potential but required careful tuning of the radius and could result in unclassified samples.

## âš™ï¸ Challenges and Decisions
â€¢â   â RNN's performance depended heavily on the selected radius. Initially, many predictions returned â â€¯-1â€¯â  (unclassified), so we added logic to filter these out during evaluation.
â€¢â   â Selecting appropriate values for â â€¯kâ€¯â  and â â€¯radiusâ€¯â  required iterative testing. We chose commonly used benchmark values for demonstration.
â€¢â   â The wine datasetâ€™s small size made it ideal for quick experimentation but also required careful handling to avoid overfitting.
